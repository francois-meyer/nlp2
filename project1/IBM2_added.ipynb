{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Python Version: 3.6\n",
    "\n",
    "Implementations of IBM models 1 and 2.\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "from math import floor\n",
    "from __future__ import division\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "NULL_TOKEN = \"<NULL>\"\n",
    "LIMIT = 100 # how sentences to train on\n",
    "\n",
    "def preprocess(line):\n",
    "    \"\"\"\n",
    "    Apply preprocessing to line in corpus.\n",
    "    :param line:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    line = line.lower()  # to lower case\n",
    "    line = re.sub(r\"\\d+\", \"\", line)  # remove digits\n",
    "    line = re.sub(r'[^\\w\\s]', \"\", line)  # remove all non-alphanumeric and non-space characters\n",
    "    line = re.sub(r\"\\s+\", \" \", line).strip()  # remove excess white spaces\n",
    "    return line\n",
    "\n",
    "\n",
    "def get_vocab(file):\n",
    "    \"\"\"\n",
    "    Extract all unique words from a corpus.\n",
    "    :param file: text file containing corpus\n",
    "    :return: set of unique words\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = set()\n",
    "    count = 0\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = preprocess(line)\n",
    "            for word in line.split():\n",
    "                vocab.add(word)\n",
    "            count += 1\n",
    "            if count == LIMIT:\n",
    "                break\n",
    "    return vocab\n",
    "\n",
    "def get_corpus(e_file, f_file):\n",
    "\n",
    "    fe = open(e_file)\n",
    "    ff = open(f_file)\n",
    "    count = 0\n",
    "    for e_sent, f_sent in zip(fe, ff):\n",
    "\n",
    "        e_sent = preprocess(e_sent)\n",
    "        e_sent = NULL_TOKEN + \" \" + e_sent\n",
    "        f_sent = preprocess(f_sent)\n",
    "        yield (e_sent.split(), f_sent.split())\n",
    "\n",
    "        count += 1\n",
    "        if count == LIMIT:\n",
    "            break\n",
    "\n",
    "class IBM(object):\n",
    "\n",
    "    def __init__(self, model=2, initialization=\"uniform\"):\n",
    "        self.model = model\n",
    "        self.t = None\n",
    "        self.e_vocab = None\n",
    "        self.f_vocab = None\n",
    "        self.initialization = initialization\n",
    "        self.max_jump = 100\n",
    "        self.jump = None\n",
    "\n",
    "    def train(self, e_file=\"training/hansards.36.2.e\", f_file=\"training/hansards.36.2.f\", iters=10):\n",
    "\n",
    "        logging.info(\"Creating English vocabulary...\")\n",
    "        self.e_vocab = get_vocab(e_file)\n",
    "        self.e_vocab.add(NULL_TOKEN)\n",
    "\n",
    "        logging.info(\"Creating French vocabulary...\")\n",
    "        self.f_vocab = get_vocab(f_file)\n",
    "\n",
    "        logging.info(\"Initialising model parameters...\")\n",
    "        self.initialise_params()\n",
    "\n",
    "        logging.info(\"Training parameters with EM...\")\n",
    "        self.EM(e_file, f_file, iters)\n",
    "\n",
    "\n",
    "    def EM(self, e_file, f_file, iters):\n",
    "\n",
    "        # Train parameters with EM algorithm\n",
    "        for iteration in range(iters):\n",
    "\n",
    "\n",
    "            if self.model == 1:\n",
    "\n",
    "                # All counts to zero for the new iteration\n",
    "                pair_counts = defaultdict(float)\n",
    "                word_counts = defaultdict(float)\n",
    "\n",
    "                # Expectation step\n",
    "                for e_sent, f_sent in get_corpus(e_file, f_file):\n",
    "\n",
    "                    normalise = {}\n",
    "                    for f_word in f_sent:\n",
    "\n",
    "                        # Sum translation probabilities of f words over all e words\n",
    "                        normalise[f_word] = 0.0\n",
    "                        for e_word in e_sent:\n",
    "                            normalise[f_word] += self.t[e_word][f_word]\n",
    "\n",
    "                        # Update counts\n",
    "                        for e_word in e_sent:\n",
    "                            delta = self.t[e_word][f_word] / normalise[f_word]\n",
    "                            pair_counts[(e_word, f_word)] += delta\n",
    "                            word_counts[e_word] += delta\n",
    "\n",
    "                # Maximisation step\n",
    "                for e_word in self.e_vocab:\n",
    "                    for f_word in self.f_vocab:\n",
    "                        #print(pair_counts[(e_word, f_word)])\n",
    "                        self.t[e_word][f_word] = pair_counts[(e_word, f_word)] / word_counts[e_word]\n",
    "\n",
    "            elif self.model == 2:\n",
    "                # EM for IBM2\n",
    "                pair_counts = defaultdict(float)\n",
    "                word_counts = defaultdict(float)\n",
    "                jump_counts = np.zeros((1, 2 * self.max_jump), dtype=np.float)\n",
    "                \n",
    "                l = len(self.e_vocab)\n",
    "                m = len(self.f_vocab)\n",
    "                # Expectation step\n",
    "                for e_sent, f_sent in get_corpus(e_file, f_file):\n",
    "\n",
    "                    normalise = {}\n",
    "                    #french word position j\n",
    "                    #english word position i \n",
    "                    i, j = 0, 0\n",
    "                    \n",
    "                    for f_word in f_sent:\n",
    "                        # Sum translation probabilities of f words over all e words\n",
    "                        normalise[f_word] = 0.0\n",
    "                        for e_word in e_sent:\n",
    "                            normalise[f_word] += self.t[e_word][f_word] * self.jump[0, self.get_jump(i, j, l, m)]\n",
    "                            i += 1\n",
    "\n",
    "                        # Update counts\n",
    "                        for e_word in e_sent:\n",
    "                            idx = self.get_jump(i, j, l, m)\n",
    "\n",
    "                        if normalise[f_word] == 0:\n",
    "                            print('help!')\n",
    "                            delta = 0\n",
    "                        else:\n",
    "                            delta = (self.t[e_word][f_word] * self.jump[0, idx]) / normalise[f_word]\n",
    "                          #  print(delta)\n",
    "\n",
    "                            pair_counts[(e_word, f_word)] += delta\n",
    "                            word_counts[e_word] += delta\n",
    "                            jump_counts[0, idx] += delta\n",
    "                            i += 1\n",
    "                            \n",
    "                        j += 1\n",
    "                        \n",
    "                # Maximisation step\n",
    "                for e_word in self.e_vocab:\n",
    "                    for f_word in self.f_vocab:\n",
    "                       # print(word_counts[e_word])\n",
    "                        #print(pair_counts[(e_word, f_word)])\n",
    "                        self.t[e_word][f_word] = pair_counts[(e_word, f_word)] / word_counts[e_word]\n",
    "                self.jump = 1./float(np.sum(jump_counts)) * jump_counts\n",
    "\n",
    "\n",
    "    def initialise_params(self):\n",
    "\n",
    "        if self.model == 1:\n",
    "\n",
    "            # Store t(f|e) as t[e][f]\n",
    "            initial_value = 1.0/len(self.f_vocab)\n",
    "            self.t = {e_word: {f_word: initial_value for f_word in self.f_vocab} for e_word in self.e_vocab}\n",
    "\n",
    "\n",
    "        elif self.model == 2:\n",
    "        # Initialise IBM2 parameters (some of which will be the same)\n",
    "            if self.initialization == \"uniform\":\n",
    "                # Store t(f|e) as t[e][f]\n",
    "                initial_value = 1.0/len(self.f_vocab)\n",
    "                self.t = {e_word: {f_word: initial_value for f_word in self.f_vocab} for e_word in self.e_vocab}\n",
    "                \n",
    "            elif self.initialization == \"random\":\n",
    "                # random samples from Dirichlet distribution\n",
    "                #alpha = (0.1,) * len(self.f_vocab)\n",
    "                #initial_value = dirichlet(alpha, size=len(self.e_vocab)).T\n",
    "                self.t = {e_word: {f_word: random.uniform(0.1,0.9) for f_word in self.f_vocab} for e_word in self.e_vocab}\n",
    "            \n",
    "            #initializing jump\n",
    "            self.jump = 1. / (2 * self.max_jump) * np.ones((1, 2 * self.max_jump), dtype = np.float)\n",
    "        \n",
    "    def get_jump(self, i, j, l, m):\n",
    "        \"\"\"\n",
    "        Align french word j to english word i. \n",
    "        Returns value in range [0, 2*max_jump] instead of [-max_jump, max_jump]\n",
    "        to get sensible indices.\n",
    "        \"\"\"\n",
    "        jump = int(i - floor(j * l / m)) + self.max_jump \n",
    "        if jump >= 2 * self.max_jump:\n",
    "            return self.max_jump - 1\n",
    "        if jump < 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return jump\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = IBM()\n",
    "    #model.train(e_file=\"mock/e\", f_file=\"mock/f\", iters=100)\n",
    "    model.train()\n",
    "\n",
    "    #print(model.t['b']['x'])\n",
    "    print(model.t)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
