{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating English vocabulary...\n",
      "INFO:root:Creating French vocabulary...\n",
      "INFO:root:Initialising model parameters...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Python Version: 3.6\n",
    "Implementations of IBM models 1 and 2.\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from aer import *\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "NULL_TOKEN = \"<NULL>\"\n",
    "LIMIT = 0# how many sentences to train on\n",
    "\n",
    "def preprocess(line):\n",
    "    \"\"\"\n",
    "    Apply preprocessing to line in corpus.\n",
    "    :param line:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    #line = line.lower()  # to lower case\n",
    "    #line = re.sub(r\"\\d+\", \"\", line)  # remove digits\n",
    "    #line = re.sub(r'[^\\w\\s]', \"\", line)  # remove all non-alphanumeric and non-space characters\n",
    "    #line = re.sub(r\"\\s+\", \" \", line).strip()  # remove excess white spaces\n",
    "    return line\n",
    "\n",
    "\n",
    "def get_vocab(file):\n",
    "    \"\"\"\n",
    "    Extract all unique words from a corpus.\n",
    "    :param file: text file containing corpus\n",
    "    :return: set of unique words\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = set()\n",
    "    count = 0\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            #line = preprocess(line)\n",
    "            for word in line.split():\n",
    "                vocab.add(word)\n",
    "            count += 1\n",
    "            if count == LIMIT:\n",
    "                break\n",
    "    return vocab\n",
    "\n",
    "def get_corpus(e_file, f_file, add_null=True):\n",
    "\n",
    "\n",
    "    fe = open(e_file)\n",
    "    ff = open(f_file)\n",
    "    count = 0\n",
    "    for e_sent, f_sent in zip(fe, ff):\n",
    "\n",
    "        #e_sent = preprocess(e_sent)\n",
    "        if add_null:\n",
    "            e_sent = NULL_TOKEN + \" \" + e_sent\n",
    "        #f_sent = preprocess(f_sent)\n",
    "        yield (e_sent.split(), f_sent.split())\n",
    "\n",
    "        count += 1\n",
    "        if count == LIMIT:\n",
    "            break\n",
    "\n",
    "class IBM(object):\n",
    "\n",
    "    def __init__(self, model=2, initialization=\"uniform\"):\n",
    "        self.model = model\n",
    "        self.t = None\n",
    "        self.e_vocab = None\n",
    "        self.f_vocab = None\n",
    "        self.valid = None\n",
    "        self.plot = False\n",
    "        self.test = False\n",
    "        self.log_likelihoods = []\n",
    "        self.valid_aers = []\n",
    "\n",
    "        self.convergence_test_aer = None\n",
    "        self.convergence_iter = None\n",
    "\n",
    "        self.best_valid_test_aer = None\n",
    "        self.best_valid_aer = 1.01\n",
    "        self.best_valid_iter = None\n",
    "        \n",
    "        self.initialization = initialization\n",
    "        self.max_jump = 100\n",
    "        self.jump = None\n",
    "\n",
    "    def train(self, e_file=\"training/hansards.36.2.e\", f_file=\"training/hansards.36.2.f\", iters=10,\n",
    "              valid=True, plot=True, test=True):\n",
    "\n",
    "        self.valid = valid\n",
    "        self.plot = plot\n",
    "        self.test = test\n",
    "\n",
    "        logging.info(\"Creating English vocabulary...\")\n",
    "        self.e_vocab = get_vocab(e_file)\n",
    "        self.e_vocab.add(NULL_TOKEN)\n",
    "\n",
    "        logging.info(\"Creating French vocabulary...\")\n",
    "        self.f_vocab = get_vocab(f_file)\n",
    "\n",
    "        logging.info(\"Initialising model parameters...\")\n",
    "        self.initialise_params()\n",
    "\n",
    "        logging.info(\"Training parameters with EM...\")\n",
    "        self.EM(e_file, f_file, iters)\n",
    "\n",
    "        if self.test:\n",
    "\n",
    "            test_aer = None\n",
    "\n",
    "            # Check if convergence AER has been set, if not then set it\n",
    "            if self.convergence_test_aer is None:\n",
    "                test_aer = self.get_aer(e_file=\"testing/test/test.e\",\n",
    "                                        f_file=\"testing/test/test.f\",\n",
    "                                        align_file=\"testing/answers/test.wa.nonullalign\",\n",
    "                                        output=True,\n",
    "                                        selection=\"convergence\")\n",
    "                self.convergence_test_aer = test_aer\n",
    "                self.convergence_iter = iters\n",
    "\n",
    "            # Check if the best validation AER has been set, if not then set it\n",
    "            if self.best_valid_test_aer is None:\n",
    "                test_aer = self.get_aer(e_file=\"testing/test/test.e\",\n",
    "                                        f_file=\"testing/test/test.f\",\n",
    "                                        align_file=\"testing/answers/test.wa.nonullalign\",\n",
    "                                        output=True,\n",
    "                                        selection=\"validation\")\n",
    "                self.best_valid_test_aer = test_aer\n",
    "                self.best_valid_iter = iters\n",
    "\n",
    "            if test_aer is None:\n",
    "                test_aer = self.get_aer(e_file=\"testing/test/test.e\",\n",
    "                                        f_file=\"testing/test/test.f\",\n",
    "                                        align_file=\"testing/answers/test.wa.nonullalign\")\n",
    "\n",
    "            logging.info(\"Final test AER: \" + str(test_aer))\n",
    "\n",
    "            logging.info(\"Selected models:\")\n",
    "\n",
    "            logging.info(\"Training log likelihood converged at iteration \" + str(self.convergence_iter))\n",
    "            logging.info(\"Test AER:\" + str(self.convergence_test_aer))\n",
    "\n",
    "            logging.info(\"Best validation AER obtained at iteration \" + str(self.best_valid_iter))\n",
    "            logging.info(\"Test AER:\" + str(self.best_valid_test_aer))\n",
    "\n",
    "        if self.plot:\n",
    "            iterations = list(range(1, len(self.log_likelihoods)+1))\n",
    "            if len(self.log_likelihoods) > iters:\n",
    "                iterations = [0] + iterations\n",
    "                iterations = iterations[:-1]\n",
    "\n",
    "            ax = sns.lineplot(iterations, self.log_likelihoods)\n",
    "            ax.set_xlabel(xlabel=\"Training iterations\", fontsize=14)\n",
    "            ax.set_ylabel(ylabel=\"Training log likelihood\", fontsize=14)\n",
    "            plt.title(\"IBM Model \" + str(self.model) + \" - Evolution of the training log likelihood\", fontsize=14)\n",
    "            plt.plot(self.convergence_iter, self.log_likelihoods[self.convergence_iter -1], \"r.\", markersize=15)\n",
    "            plt.savefig(\"train_ibm\" + str(self.model))\n",
    "            plt.show()\n",
    "\n",
    "            ax = sns.lineplot(iterations, self.valid_aers)\n",
    "            ax.set_xlabel(xlabel=\"Training iterations\", fontsize=14)\n",
    "            ax.set_ylabel(ylabel=\"AER on validation data\", fontsize=14)\n",
    "            plt.title(\"IBM Model \" + str(self.model) + \" - Evolution of the validation AER\", fontsize=14)\n",
    "            plt.plot(self.best_valid_iter, self.valid_aers[self.best_valid_iter - 1], \"r.\", markersize=15)\n",
    "            plt.savefig(\"valid_ibm\" + str(self.model))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def EM(self, e_file, f_file, iters):\n",
    "\n",
    "        # Train parameters with EM algorithm\n",
    "        for i in range(iters):\n",
    "\n",
    "            logging.info(\"Starting iteration \" + str(i+1))\n",
    "\n",
    "            if self.model == 1:\n",
    "\n",
    "                # All counts to zero for the new iteration\n",
    "                pair_counts = defaultdict(float)\n",
    "                word_counts = defaultdict(float)\n",
    "\n",
    "                # Expectation step\n",
    "                logging.info(\"Expectation step\")\n",
    "                for e_sent, f_sent in get_corpus(e_file, f_file):\n",
    "\n",
    "                    normalise = {}\n",
    "                    for f_word in f_sent:\n",
    "\n",
    "                        # Sum translation probabilities of f words over all e words\n",
    "                        normalise[f_word] = 0.0\n",
    "                        for e_word in e_sent:\n",
    "                            normalise[f_word] += self.t[e_word][f_word]\n",
    "\n",
    "                        # Update counts\n",
    "                        for e_word in e_sent:\n",
    "                            delta = self.t[e_word][f_word] / normalise[f_word]\n",
    "                            pair_counts[(e_word, f_word)] += delta\n",
    "                            word_counts[e_word] += delta\n",
    "\n",
    "                # Maximisation step\n",
    "                logging.info(\"Maximisation step\")\n",
    "                for e_sent, f_sent in get_corpus(e_file, f_file):\n",
    "                    for e_word in e_sent:\n",
    "                        for f_word in f_sent:\n",
    "                            self.t[e_word][f_word] = pair_counts[(e_word, f_word)] / word_counts[e_word]\n",
    "\n",
    "            elif self.model == 2:\n",
    "                # EM for IBM2\n",
    "                pair_counts = defaultdict(float)\n",
    "                word_counts = defaultdict(float)\n",
    "                jump_counts = np.zeros((1, 2 * self.max_jump), dtype=np.float)\n",
    "                \n",
    "                l = len(self.e_vocab)\n",
    "                m = len(self.f_vocab)\n",
    "                # Expectation step\n",
    "                for e_sent, f_sent in get_corpus(e_file, f_file):\n",
    "\n",
    "                    normalise = {}\n",
    "                    #french word position j\n",
    "                    #english word position i \n",
    "                    \n",
    "                    for j, f_word in enumerate(f_sent):\n",
    "                        # Sum translation probabilities of f words over all e words\n",
    "                        normalise[f_word] = 0.0\n",
    "                        for i, e_word in enumerate(e_sent):\n",
    "                            normalise[f_word] += self.t[e_word][f_word] * self.jump[0, self.get_jump(i, j, l, m)]\n",
    "\n",
    "                        # Update counts\n",
    "                        for i, e_word in enumerate(e_sent):\n",
    "                            idx = self.get_jump(i, j, l, m)\n",
    "                          #  if normalise[f_word] == 0:\n",
    "                          #      print('help!')\n",
    "                          #      delta = 0\n",
    "                          #  else:\n",
    "                            delta = (self.t[e_word][f_word] * self.jump[0, idx]) / normalise[f_word]\n",
    "                            \n",
    "                            pair_counts[(e_word, f_word)] += delta\n",
    "                            word_counts[e_word] += delta\n",
    "                            jump_counts[0, idx] += delta\n",
    "\n",
    "                # Maximisation step\n",
    "                for e_sent, f_sent in get_corpus(e_file, f_file):\n",
    "                    for e_word in e_sent:\n",
    "                        for f_word in f_sent:\n",
    "                            self.t[e_word][f_word] = pair_counts[(e_word, f_word)] / word_counts[e_word]\n",
    "                self.jump = 1./float(np.sum(jump_counts)) * jump_counts\n",
    "\n",
    "                \n",
    "            logging.info(\"Complete\")\n",
    "            if self.valid:\n",
    "                # Compute and store training log likelihood\n",
    "                log_likelihood = self.get_log_likelihood(e_file, f_file)\n",
    "                self.log_likelihoods.append(log_likelihood)\n",
    "                logging.info(\"Training log likelihood: \" + str(log_likelihood))\n",
    "\n",
    "                # Check if training log likelihood has converged\n",
    "                if self.convergence_test_aer is None and len(self.log_likelihoods) >= 2 and self.log_likelihoods[-1] < 1.001 * self.log_likelihoods[-2]:\n",
    "                    test_aer = self.get_aer(e_file=\"testing/test/test.e\",\n",
    "                                            f_file=\"testing/test/test.f\",\n",
    "                                            align_file=\"testing/answers/test.wa.nonullalign\",\n",
    "                                            output=True,\n",
    "                                            selection=\"convergence\")\n",
    "                    self.convergence_test_aer = test_aer\n",
    "                    self.convergence_iter = i + 1\n",
    "\n",
    "                # Compute and store validation AER\n",
    "                valid_aer = self.get_aer() # on validation set\n",
    "                self.valid_aers.append(valid_aer)\n",
    "                logging.info(\"Validation AER: \" + str(valid_aer))\n",
    "\n",
    "                # Check if the current validation AER is the best so far\n",
    "                if self.best_valid_aer > valid_aer:\n",
    "                    self.best_valid_aer = valid_aer\n",
    "                    test_aer = self.get_aer(e_file=\"testing/test/test.e\",\n",
    "                                            f_file=\"testing/test/test.f\",\n",
    "                                            align_file=\"testing/answers/test.wa.nonullalign\",\n",
    "                                            output=True,\n",
    "                                            selection=\"validation\")\n",
    "                    self.best_valid_test_aer = test_aer\n",
    "                    self.best_valid_iter = i + 1\n",
    "\n",
    "\n",
    "\n",
    "    def get_aer(self, e_file=\"validation/dev.e\", f_file=\"validation/dev.f\", align_file=\"validation/dev.wa.nonullalign\",\n",
    "                    output=False, selection=\"validation\"):\n",
    "\n",
    "        gold_sets = read_naacl_alignments(align_file)\n",
    "\n",
    "        # 2. Here you would have the predictions of your own algorithm\n",
    "        predictions = []\n",
    "        for e_sent, f_sent in get_corpus(e_file, f_file):\n",
    "\n",
    "            # For each french word, find the most likely aligned english word\n",
    "            links = set()\n",
    "            for f_index, f_word in enumerate(f_sent):\n",
    "                max_t = 0.0\n",
    "                max_a = 0 # assume null aligned\n",
    "                for e_index, e_word in enumerate(e_sent):\n",
    "                    if self.t[e_word][f_word] > max_t:\n",
    "                        max_t = self.t[e_word][f_word]\n",
    "                        max_a = e_index\n",
    "\n",
    "                if max_a != 0: # Not aligned to NULL word\n",
    "                    link = (max_a, f_index+1)\n",
    "                    links.add(link)\n",
    "\n",
    "            predictions.append(links)\n",
    "\n",
    "        # 3. Compute AER\n",
    "        metric = AERSufficientStatistics()\n",
    "        for gold, pred in zip(gold_sets, predictions):\n",
    "            metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "\n",
    "        if output:\n",
    "            logging.info(\"Writing predicted test alignments to file.\")\n",
    "            file_name = \"ibm\" + str(self.model) + \".mle.naacl_\" + selection\n",
    "\n",
    "            # Clear file\n",
    "            open(file_name, 'w').close()\n",
    "\n",
    "            # Write predictions to file\n",
    "            with open(file_name, 'a') as file:\n",
    "                for i, pred in enumerate(predictions):\n",
    "                    for link in pred:\n",
    "                        file.write(str(i+1).zfill(4) + \" \" + str(link[0]) + \" \" + str(link[1]) + \" S\\n\")\n",
    "\n",
    "        return metric.aer()\n",
    "\n",
    "\n",
    "    def get_log_likelihood(self, e_file, f_file):\n",
    "\n",
    "        \"\"\"TODO: Maybe add normalisation for alignment probabilities\"\"\"\n",
    "\n",
    "        log_likelihood = 0.0\n",
    "\n",
    "        for e_sent, f_sent in get_corpus(e_file, f_file):\n",
    "            sentence_likelihood = 0.0\n",
    "            for f_word in f_sent:\n",
    "                for e_word in e_sent:\n",
    "                    sentence_likelihood += self.t[e_word][f_word]\n",
    "\n",
    "            if sentence_likelihood > 0.0:\n",
    "                log_likelihood += np.log(sentence_likelihood)\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def initialise_params(self):\n",
    "\n",
    "        if self.model == 1:\n",
    "\n",
    "            # Store t(f|e) as t[e][f]\n",
    "            initial_value = 1.0/len(self.f_vocab)\n",
    "            #self.t = {e_word: {f_word: initial_value for f_word in self.f_vocab} for e_word in self.e_vocab}\n",
    "            self.t = defaultdict(lambda: defaultdict(lambda: initial_value))\n",
    "\n",
    "        elif self.model == 2:\n",
    "        # Initialise IBM2 parameters (some of which will be the same)\n",
    "            if self.initialization == \"uniform\":\n",
    "                # Store t(f|e) as t[e][f]\n",
    "                initial_value = 1.0/len(self.f_vocab)\n",
    "                self.t = {e_word: {f_word: initial_value for f_word in self.f_vocab} for e_word in self.e_vocab}\n",
    "                \n",
    "            elif self.initialization == \"random\":\n",
    "                # random samples from Dirichlet distribution\n",
    "                #alpha = (0.1,) * len(self.f_vocab)\n",
    "                #initial_value = dirichlet(alpha, size=len(self.e_vocab)).T\n",
    "                self.t = {e_word: {f_word: random.uniform(0.1,0.9) for f_word in self.f_vocab} for e_word in self.e_vocab}\n",
    "            \n",
    "            #initializing jump\n",
    "            self.jump = 1. / (2 * self.max_jump) * np.ones((1, 2 * self.max_jump), dtype = np.float)\n",
    "\n",
    "    def get_jump(self, i, j, l, m):\n",
    "        \"\"\"\n",
    "        Align french word j to english word i. \n",
    "        Returns value in range [0, 2*max_jump] instead of [-max_jump, max_jump]\n",
    "        to get sensible indices.\n",
    "        \"\"\"\n",
    "        jump = int(i - floor(j * l / m)) + self.max_jump \n",
    "        if jump >= 2 * self.max_jump:\n",
    "            return self.max_jump - 1\n",
    "        if jump < 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return jump\n",
    "\n",
    "def main():\n",
    "    model = IBM()\n",
    "    #model.train(e_file=\"mock/e\", f_file=\"mock/f\", iters=100)\n",
    "    model.train(iters=1)\n",
    "\n",
    "    #print(model.t['b']['x'])\n",
    "    #print(model.t)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
